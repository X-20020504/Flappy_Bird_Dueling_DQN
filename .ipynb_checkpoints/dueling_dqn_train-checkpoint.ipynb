{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d283745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b416e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VA_Network(nn.Module):\n",
    "    # 用于Dueling DQN的V/A网\n",
    "    def __init__(self, hidden_dim, action_dim):\n",
    "        super(VA_Network, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "        self.fc1 = torch.nn.Linear(1000, hidden_dim)\n",
    "        self.fc_A = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_V = torch.nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        A = self.fc_A(x)\n",
    "        V = self.fc_V(x)\n",
    "        Q = V + A - A.mean(1).view(-1, 1)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c739f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "class Dueling_DQN:\n",
    "    # 图片为输入的Dueling DQN算法\n",
    "    # 图片的输入尺寸是(batch_size, 3, 224, 224)\n",
    "    def __init__(self, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon  # epsilon-greedy阈值\n",
    "        self.target_update = target_update\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "        self.q_net = VA_Network(hidden_dim=self.hidden_dim, action_dim=self.action_dim).to(device)\n",
    "        self.target_q_net = VA_Network(hidden_dim=self.hidden_dim, action_dim=self.action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def CNN_forward(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).to(self.device)\n",
    "        state = state.permute([2, 0, 1])\n",
    "        transform = torchvision.transforms.Resize((224, 224))\n",
    "        state = transform(state)\n",
    "        state = state.unsqueeze(0)\n",
    "        state = state / 255\n",
    "        return self.q_net(state)\n",
    "        \n",
    "    def take_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            action = self.CNN_forward(state).argmax().item()\n",
    "        return action\n",
    "    \n",
    "    def max_q_value(self, state):\n",
    "        return self.CNN_forward(state).max().item()\n",
    "    \n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        \n",
    "        # ======================================================================\n",
    "#         print(\"states_shape:\", states.shape)\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "#         print(\"next_states_shape:\", next_states.shape)\n",
    "        max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1)\n",
    "        \n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))\n",
    "        self.optimizer.zero_grad()\n",
    "        dqn_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:  # 将决策网络参数同步给目标网络\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b07049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_to_con(discrete_action, env, action_dim):  # 离散动作转回连续的函数\n",
    "    action_lowbound = env.action_space.low[0]  # 连续动作的最小值\n",
    "    action_upbound = env.action_space.high[0]  # 连续动作的最大值\n",
    "    return action_lowbound + (discrete_action /\n",
    "                              (action_dim - 1)) * (action_upbound -\n",
    "                                                   action_lowbound)\n",
    "\n",
    "def train_DQN(agent, env, num_episodes, replay_buffer, minimal_size,\n",
    "              batch_size):\n",
    "    return_list = []\n",
    "    max_q_value_list = []\n",
    "    max_q_value = 0\n",
    "    for i in range(10):\n",
    "        with tqdm(total=int(num_episodes / 10),\n",
    "                  desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):\n",
    "                episode_return = 0\n",
    "                state, info = env.reset()\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = agent.take_action(state)\n",
    "                    max_q_value = agent.max_q_value(\n",
    "                        state) * 0.005 + max_q_value * 0.995  # 平滑处理\n",
    "                    max_q_value_list.append(max_q_value)  # 保存每个状态的最大Q值\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    \n",
    "                    state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "                    state_tensor = state_tensor.permute([2, 0, 1])\n",
    "                    transform = torchvision.transforms.Resize((224, 224))\n",
    "                    state_tensor = transform(state_tensor)\n",
    "                    state_tensor = state_tensor / 255\n",
    "                    \n",
    "                    next_state_tensor = torch.tensor(next_state, dtype=torch.float)\n",
    "                    next_state_tensor = next_state_tensor.permute([2, 0, 1])\n",
    "                    transform = torchvision.transforms.Resize((224, 224))\n",
    "                    next_state_tensor = transform(next_state_tensor)\n",
    "                    next_state_tensor = next_state_tensor / 255\n",
    "                    \n",
    "                    replay_buffer.add(state_tensor.numpy(), action, reward, next_state_tensor.numpy(), done)\n",
    "                    \n",
    "                    state = next_state\n",
    "                    episode_return += reward\n",
    "                    if replay_buffer.size() == minimal_size:\n",
    "                        print(\"Now Start Training!\")\n",
    "                    if replay_buffer.size() > minimal_size:\n",
    "                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(\n",
    "                            batch_size)\n",
    "                        transition_dict = {\n",
    "                            'states': b_s,\n",
    "                            'actions': b_a,\n",
    "                            'next_states': b_ns,\n",
    "                            'rewards': b_r,\n",
    "                            'dones': b_d\n",
    "                        }\n",
    "                        agent.update(transition_dict)\n",
    "                return_list.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                        '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "    return return_list, max_q_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frank/opt/anaconda3/envs/rl-gym/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/frank/opt/anaconda3/envs/rl-gym/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "Iteration 0:   0%|                                       | 0/10 [00:00<?, ?it/s]/Users/frank/opt/anaconda3/envs/rl-gym/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Start Training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration 0:  10%|██▋                        | 1/10 [19:13<2:53:02, 1153.57s/it]"
     ]
    }
   ],
   "source": [
    "import rl_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "hidden_dim = 512\n",
    "action_dim = 4\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.98\n",
    "epsilon = 0.01\n",
    "target_update = 50  # 目标网络更新周期\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "buffer_size = 10000\n",
    "\n",
    "agent = Dueling_DQN(hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device)\n",
    "env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\", obs_type=\"rgb\")\n",
    "num_episodes = 100\n",
    "replay_buffer = rl_utils.ReplayBuffer(buffer_size)\n",
    "minimal_size = 100  # 开始训练的回放池样本数\n",
    "batch_size = 64\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "return_list, max_q_value_list = train_DQN(agent, env, num_episodes, replay_buffer, minimal_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ba565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
